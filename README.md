# Task-8
一 文本分类
文本分类是nlp中的简单任务，已经能够取得很好的结果了。情感分类是文本分类任务中的典型问题，经典数据集是IMDb。自从有了bert，微调预训练的bert几乎可以秒杀一众模型~不过在这里，主要介绍两个模型，一个是基于rnn的，一个是基于cnn的。

note1.无论是循环神经网络，还是卷积神经网络，理论上都是能在任何长度的文本上进行运算的，对数据中的句子进行补齐或截断是不必要的，但规范化模型的长度会有利于模型进行批量化的计算，加强模型并行计算的能力

note2.设计分词函数时要注意预训练词向量是定义在什么词典上的，不能使 out-of-vocabulary 词过多

1.1 BiLSTM
双向循环神经网络比单向循环神经网络更能抓住文本的特征（由于自然语言中常常有一些特殊的倒装结构，所以拥有两个方向的隐藏状态的双向循环神经网络，其输出更具文本代表性。注意，虽然两个方向的隐藏状态在输出前被拼接在了一起，但他们仍然是独立地被运算出来的，所以双向循环神经网络也不能从根本上解决文本的双向依赖问题，而卷积神经网络和 Transformer 这样完全并行的结构则不存在该问题）

1.2 TextCNN
①一维卷积层输出的宽度为输入的宽度减去核的宽度加一

②定义多个输出通道的卷积核有利于模型提取更丰富的文本特征，定义多种宽度的卷积核有利于模型提取多个层次的文本特征（输出通道数越多，可以捕捉的单词的组合就越多；而拥有不同宽度的核，就能让模型对文本中各个长度的单词组合都有关注）

③并不需要对每一个卷积核都单独地定义一个池化层（由于池化操作与输入的序列长度无关，本身也不含任何参数，故可以所有卷积核共用一个池化层）

④对所有卷积操作的结果进行池化和拼接之后，得到的向量就可以作为文本的一个整体的表示（每个卷积和池化后的结果都可以看作是该卷积核在文本上提取出的特征，而拼接这些特征，就能得到整个文本的一个整体表示）

二 数据增强
这部分主要介绍一个计算机视觉中的常用技术，叫做图像增广(image augmentation)，它通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。

2.1 常用的图像增广方法
翻转和裁剪(常常被用到~也常常很有效)
①翻转有左右翻转(也叫水平翻转)和上下翻转，其中左右翻转图像通常不改变物体的类别，它是最早也是最广泛使用的一种图像增广方法。如果翻转会改变物体类别，那就不能翻转啦~

②裁剪，指的是通过随机裁剪来让物体以不同的比例出现在图像的不同位置，这能够降低模型对目标位置的敏感性(与池化层能降低卷积层对目标位置的敏感度的效果类似)。

变化颜色
可以从4个方面改变图像的颜色:亮度、对比度、饱和度和色调。比如将图像的亮度随机变化为原图亮度的50%(即1 − 0.5)∼ 150%(即1 + 0.5)

叠加多个图像增广方法

同时使用多种图像增广方法

2.2 注意事项
具体选择哪个增广方法要根据实际情况而定，要选的适当。如果应用场景亮度变化较大，既有白天也有黑夜，那应该进行亮度这一图像增广方式，其它的增广方式可以不用或者少用；如果实际情况对色彩变化较大，那应该进行颜色的变化；如果全部增广方式都使用的话，可能会增加模型的训练负担。
为了在预测时得到确定的结果，通常只将图像增广应用在训练样本上，而不在预测时使用含随机操作的图像增广。
2.3 图像增广的优点是什么？为什么要用图像增广？
图像增广基于现有训练数据生成随机图像从而应对过拟合

随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同 方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。

三 模型微调
有时候，我们缺乏数据，难以训练有效的模型，收集数据的成本很高，怎么办？一种可行的方法是迁移学习。迁移学习中一个很常见的技术是微调(fine-tune),在《动手学深度学习》书中是以图像识别为例进行讲解的，但实际上nlp中也有很多微调的技术。

3.1 什么是微调(fine-tune)？
将从源数据集学到的知识迁移到目标数据集上。例如，在椅子识别任务中，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。

曾经看过一篇论文，论文中实验证明，NN的前几层抽取的是通用特征，最后几层抽取的是与目标有关的特征，而中间几层是共同起作用的。如果4-6层共同起作用，那么固定前5层，微调后面几层的话，会导致效果变差，因为4、5层与第6层的交互作用被破坏了。

微调步骤：源数据集训练模型 -> 假设最后一层与目标层有关，那么对于目标模型，复制源模型前几层的参数，而最后一层的参数随机初始化 -> 微调前几层的参数，最后一层从头学习。如下图所示：

                                 

3.2 为什么使用微调(fine-tune)？
因为

如果直接用目标数据集训练模型，数据不够，若用简单模型容易效果不好，复杂模型容易过拟合

如果直接使用源数据集上训练的模型，当源数据集的数据量远大于目标数据集的话，那么对目标数据集而已，源模型是过于复杂的，容易造成对目标任务的过拟合

所以，微调那些抽取通用特征的层，从头训练那些与目标任务相关的层，是一个行之有效的方法。

 

3.3 如果做到微调(fine-tune)？
微调，顾名思义，就是对微小的调整，要是调整的多了，就会造成“遗忘灾难”，即忘掉了在源模型中学会的特征抽取模式。那怎么微调呢？下面就总结下从论文中看到的一些常用方式（多谢ykr同学的补充~）：

①学习率小自然就微调。对于需要微调的层，用较小的学习率；对其他从头开始学习的层，用较大的学习率。（该方法常用且有效，亲测可用）

②在调整之前，假设该层的参数为W0，那么在损失函数中加上一个正则项，最小化loss，自然希望该项越小越好。

③假设微调层的参数是W，那么对W的梯度进行梯度截断，梯度小了，走的步子也小了，自然也是微调。
